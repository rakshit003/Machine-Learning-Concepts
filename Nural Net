****Multi class classification in Nural net

Logistic Regression is a generalization of softmax regressign, for n=2
In order to achive multiclass in nuralnet we add a softmax function multi node only in the output layer 
a1= e^z1/e^z1+e^z2+.....e^zn   -- n is number of classes
all the other layer remain same as normal NN with 2 classes

Below is an example of how to construct this network in Tensorflow. 
Notice the output layer uses a linear rather than a softmax activation. 
While it is possible to include the softmax in the output layer, it is more numerically stable if 
linear outputs are passed to the loss function during training. 
If the model is used to predict probabilities, the softmax can be applied at that point.
**
import numpy as np
import matplotlib.pyplot as plt
%matplotlib widget
from sklearn.datasets import make_blobs
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
np.set_printoptions(precision=2)
from lab_utils_multiclass_TF import *
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)

tf.random.set_seed(1234)  # applied to achieve consistent results
model = Sequential(
    [
        Dense(2, activation = 'relu',   name = "L1"),
        Dense(4, activation = 'linear', name = "L2")
    ]
)

model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(0.01),
)

model.fit(
    X_train,y_train,
    epochs=200
)

**
breakdown of how layers work in NN
# gather the trained parameters from the first layer
l1 = model.get_layer("L1")
W1,b1 = l1.get_weights()

# plot the function of the first layer
plt_layer_relu(X_train, y_train.reshape(-1,), W1, b1, classes)

These plots show the function of Units 0 and 1 in the first layer of the network. The inputs are ( ùë•0,ùë•1
 ) on the axis. The output of the unit is represented by the color of the background. This is indicated by the color bar on the right of each graph. Notice that since these units are using a ReLu, the outputs do not necessarily fall between 0 and 1 and in this case are greater than 20 at their peaks. The contour lines in this graph show the transition point between the output,  ùëé[1]ùëó
  being zero and non-zero. Recall the graph for a ReLu : The contour line in the graph is the inflection point in the ReLu


# gather the trained parameters from the output layer
l2 = model.get_layer("L2")
W2, b2 = l2.get_weights()
# create the 'new features', the training examples after L1 transformation
Xl2 = np.maximum(0, np.dot(X_train,W1) + b1)

plt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,
                        x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1])))


**
****Difference Between Loss and Cost Function
Loss function is defined for only 1 row(example) of the data set and a give point in time gives the loss of inputted row
Cost function is for all records. Its an average of loss of each record of the dataset, 
Gradient decent plots cost function against model weights and minimise the cost by adjusting the weights slowly

*****making multi class datatset for testing

# make 4-class dataset for classification
from sklearn.datasets import make_blobs
classes = 4
m = 100
centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]
std = 1.0
X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)
plt_mc(X_train,y_train,classes, centers, std=std)
# show classes in data set


print(f"unique classes {np.unique(y_train)}")
# show how classes are represented
print(f"class representation {y_train[:10]}")
# show shapes of our dataset
print(f"shape of X_train: {X_train.shape}, shape of y_train: {y_train.shape}")


*********Learning Rate
normal for a gradient decent ,learning rate is constant, and if we have a small learning rate learning is slower
and large learning rate sometimes can not reach the global minima.

Adam algorithm provides adaptable learning rate based on situation. If learning is going in same direction slowly then it increases apha
automatically. If learning is ossilating back and forth then reduce the alpha to give it a direction.
Although it requires a default input learning rate initially.

For every weight/parameter it has a different learning rate.

******Convolution Layer*****

Unlike Normal NN layer, Convolution NN layer only looks at a part of input rather than all the indput.
like only a subsection of an image rather than the entire image. so not all links of input are attached to that layer.
Multiple convolutional layers in NN is called Convolution Nural Net

Advantages:
-Fast Computation
-Needs less training data
-Less Prone to overfitting
- This also results in another architectural choice of how many inputs a nuron should have

******RELU ACTIVATION***
The ReLU provides a continuous linear relationship. Additionally it has an 'off' range where the output is zero. 
The "off" feature makes the ReLU a Non-Linear activation. 
Why is this needed? This enables multiple units to contribute to to the resulting function without interfering.

*********Regularization*****8

It doesn't hurt to increase the size of NN so long as regularization is in place to avoid overfitting

regularization can be done at every layer with parameter Kernal_regularizer=L2(0.01)

*****Transfer learning******
Reuse the ithe initial and hidden layers of an existing NN model, like multi class classfication of differnet image
and only replace the output layer with your case specific classification. This process is called fine tuning

there are 2 options of training such algo

1. keep the copied layer parameters static and only retrain the output layer using stocastic/gradient/adam optimization
2. train the entire NN by utilizing the initialized weights of initial and hidden layers

option 1 is good when you have a small training set and option 2 when you have a large training set

Advantage is that one doese not need to label data or get many training examples. instead with small
data set we can leverage the power of pretrained models and reuse the learning from them.

This works for images because, the intial and middle layers of Image NN helps in idetifying 
feartues like edges, corners,shapes which remaing common problem to decode acorss any image

one limitation is that the input size of fine tuning should be same as the initial NN input size


******8example of building NN usecase******

https://github.com/rakshit003/gh-pages/blob/736b2918aad195c49ce65eb45d8394a577d3521b/C2_W3_Assignment%20(2).ipynb


********learnig algorithm*****

learning is done in NN using back propagation of computational graphs
where series of computations is broken down by individual arithmentic blocks and partial derivative of the cost function
is calulated backwards for each block untill we get the derivate with respect to parameters(W and b)
This  derivate is used to update the weights in next iteration

Overall computation needed is N(number of nodes) + P(number of paramters) if we use computational graphs instead of caluclating
each parameter derivatives independently , as compuational graph  helps in this calculation of derivative of each parameter
in single iteration

