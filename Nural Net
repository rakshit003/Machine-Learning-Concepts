****Multi class classification in Nural net

Logistic Regression is a generalization of softmax regressign, for n=2
In order to achive multiclass in nuralnet we add a softmax function multi node only in the output layer 
a1= e^z1/e^z1+e^z2+.....e^zn   -- n is number of classes
all the other layer remain same as normal NN with 2 classes

Below is an example of how to construct this network in Tensorflow. 
Notice the output layer uses a linear rather than a softmax activation. 
While it is possible to include the softmax in the output layer, it is more numerically stable if 
linear outputs are passed to the loss function during training. 
If the model is used to predict probabilities, the softmax can be applied at that point.
**
import numpy as np
import matplotlib.pyplot as plt
%matplotlib widget
from sklearn.datasets import make_blobs
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
np.set_printoptions(precision=2)
from lab_utils_multiclass_TF import *
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)

tf.random.set_seed(1234)  # applied to achieve consistent results
model = Sequential(
    [
        Dense(2, activation = 'relu',   name = "L1"),
        Dense(4, activation = 'linear', name = "L2")
    ]
)

model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(0.01),
)

model.fit(
    X_train,y_train,
    epochs=200
)

**
breakdown of how layers work in NN
# gather the trained parameters from the first layer
l1 = model.get_layer("L1")
W1,b1 = l1.get_weights()

# plot the function of the first layer
plt_layer_relu(X_train, y_train.reshape(-1,), W1, b1, classes)

These plots show the function of Units 0 and 1 in the first layer of the network. The inputs are ( ùë•0,ùë•1
 ) on the axis. The output of the unit is represented by the color of the background. This is indicated by the color bar on the right of each graph. Notice that since these units are using a ReLu, the outputs do not necessarily fall between 0 and 1 and in this case are greater than 20 at their peaks. The contour lines in this graph show the transition point between the output,  ùëé[1]ùëó
  being zero and non-zero. Recall the graph for a ReLu : The contour line in the graph is the inflection point in the ReLu


# gather the trained parameters from the output layer
l2 = model.get_layer("L2")
W2, b2 = l2.get_weights()
# create the 'new features', the training examples after L1 transformation
Xl2 = np.maximum(0, np.dot(X_train,W1) + b1)

plt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,
                        x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1])))


**
****Difference Between Loss and Cost Function
Loss function is defined for only 1 row(example) of the data set and a give point in time gives the loss of inputted row
Cost function is for all records. Its an average of loss of each record of the dataset, 
Gradient decent plots cost function against model weights and minimise the cost by adjusting the weights slowly

*****making multi class datatset for testing

# make 4-class dataset for classification
from sklearn.datasets import make_blobs
classes = 4
m = 100
centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]
std = 1.0
X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)
plt_mc(X_train,y_train,classes, centers, std=std)
# show classes in data set


print(f"unique classes {np.unique(y_train)}")
# show how classes are represented
print(f"class representation {y_train[:10]}")
# show shapes of our dataset
print(f"shape of X_train: {X_train.shape}, shape of y_train: {y_train.shape}")


*********Learning Rate
normal for a gradient decent ,learning rate is constant, and if we have a small learning rate learning is slower
and large learning rate sometimes can not reach the global minima.

Adam algorithm provides adaptable learning rate based on situation. If learning is going in same direction slowly then it increases apha
automatically. If learning is ossilating back and forth then reduce the alpha to give it a direction.

For every weight/parameter it has a different learning rate.

