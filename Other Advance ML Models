************Decision Tree************

****Classification Decision Tree****

Idea is to maximize Purity during everfy node split. Purity means increasing the presense of a given class in the mode.
Or minimize Impurtity(Entropy). Hence close the mix of 2 classes is to 0.5 its more impure and if a fraction of presence of 
one class close to 1 or close to 0 makes it more pure and reduce entropy

Decision tree is a recursive alogirthm instead of gradient decent learning

Hence in decision tree we minimize Entropy (Impurity fucntion) instead of cost function to learn the model
Entropy= -plog(p) - (1-p)log(1-p).. if we plot this its a inverted parabola with peak at p=0.5
p is the fraction of instance of a class

Even Gini function also looks like this

In actual decision tree, entroy of both the nodes are done a weighted average so the bigger sample node gets more importance
and the informaation gain is caculated by butracting the root node entropy with the wirghted average chid node entropy

Information gain is calculated firs for every feature and then among different feature to decide
which feature and what value of the feature should be used for the split


if the ctaegorical feature has multiple values then its better to use one hot encoding, which means for very category
we create a new feature and mark it as 1 where that category exist and 0 where is does not.
for a give row one of the feature will have a value 1, which means that feature is hot for the record.
Hence the name one hot encoding.Advantage is for spliting a node it will make a binary decision for each feature

This is also work with NN or Logistic or linear regression training

In case of continuous feature, data of n points, choose n-1 midpoints between n successive points and find
information gain at that point, as a result fin the midpoint with the most information gain


*****Regression Decision Tree*****

In Regression tree the value of a node is decised by the average value of the priction numeri variable of the samples in that node
for learning process Instead of Entropy(impurity) , variance of the node is used as a threshold. feature which results in least
weighted variance is use to decide.

And instead of information gain from root node. variance lost between is used as a measure by algorithm

***** Ensamle of Trees****

Issue with single decision tree is that even if change make changes to minority of examples. the tree scruture can change
as a new feature can become the main root node of the tree . Hence its very sensitive of change in input.
Where as if we train a bunch of decision trea and use a voting classifier to get the predictions , it turns out to be more
stable.

*****Method== Sampling with Replacement*****
create multiple data set for different tree by fecthing samples with replacement (this will result in duplicate training examples which is fine)
As the data set changes ,with that the feature of the node also change as the new entropy might be different because of the new variation in dataset

multiple ways to ensamble

Random Forrest: 
Sample with replacement and create same size dataset B times to create B trees.  B  typically could be 64-128.
This is also called Bagged Decision Tree. This concept is further extended by not chosing from all features avialable
but instead only choose from n<N features becasue this will help in creating different type of decision trees.

XG Boost: Xtreme gradient boosting
Idea is to instead of picking the samples in the new mag randomly, give more probablity of picking to 
miscalssified examples from periously built trees, hence the subsecquent tree built will have training set of more samples of misclassified 
records from ensamle of built so far. This process is called boosting

XG boost is very good for prediction in lines with NN


*****Clustering*******

Clustering algorithm looks at a set of data points and automatically finds datapoints that are related to each other
Use cases: grouping similar articales together, Analyse DNA data to group people into similar traits, Astronomers use to group
bodies in space

K means Clsutering: 
-randomly pick multiple centors of the clusters
-It does 2 things repeatedly , 1 Assign each point to centroids , 2 move/update the cluster centroids

Assigning points to cluster centroids is decided by how closer point is to which cluster
After the assign of points to initial centroids , an average of each group is taken and that becomes the new centroid

Above 2 points are repeated
Idea is afet multiple iterations when there is no further changes to location of centroids
, algorithm will converge the true centroids and group similar clusters together

Cost function is nothing but the avergae of distance error of each point with their centroids

Constraint1: Random initialization can result into centroid being stuck in local minima

In order to fix random initialization error, Run multiple random trials and let the cost function 
minimize the centroid error across all these trials. which ever centroid errors is the minimum chose those as
true centroids.

Constraint2: How to choose number of clusters

Elbow Cureve:One method is by plotting Cost function agains different values of and where ever we get eblow joint
we take that value.
But normally if you plot this curve, J will always reduce with higher and higher value of k , which also results 
into overfitting

A better way to pick k is from business requirement and cost of splitting into more k for the buesiness


**Anomali detection***

**Gaussian Model***
Density Estimation using gaussian function for each variable
Estimate Mu and Sigma2 for each variable to and multiply to find the overall probablity function

Anomaly detection tries to find brand new positive examples that may be unlike anything you've seen before. 
Where supervised learning looks at your positive examples and tries to decide if a future example is similar to the positive examples that you've already seen.

Many security related applications will use anomaly detection as hackers will try to come up new ways of comminting breach.
Whereas returning to supervised learning, if you want to learn to predict the weather well, there's only a handful types of weather that you typically see.
Is it sunny, rainy, is it going to snow

Chosing feature for anomaly detection is even more important as it will not auto rule out that feature by assign low weight (in supervised)
Its better to chose a feature that is more gaussian in nature for anomaly detection
If a disctrubution is skewed for a variable ,we can transform the variable as Xnew= log(X +C) or square root , or x^4 etc.
inorder to make it gaussian

Another way to imrove the anomaly detection algo is by doing error analysis, where in 
by looking at the misclassified anomalies(in CV set) and took that as inspiration and 
finding out a new features in whose gaussian distribution its anamalus

***recommendation system***

**colaborative filtering **

Colaborative algo recommends based on  user who have rated items similar to you.

A content-based filtering model will not select items if the userâ€™s previous behavior does not provide evidence for this.
Without having any variables or information about the users, just by the rating they have provided we can get inherent
characteristics of the items they have rated by views their ratings in a collaborative way

Give a a set of categories(eg Movies) to rate and given a set of users providing their rating on these movies
An Algorithm can be built that identify the inhent qualities of the movie(like genre variables). Quality variables
along with Parameter for each user get modelled to find out the individual user parameter and movies X variables

The goal of a collaborative filtering recommender system is to generate two vectors: For each user, a 'parameter vector' that embodies the movie tastes of a user. 
For each movie, a feature vector of the same size which embodies some description of the movie. 
The dot product of the two vectors plus the bias term should produce an estimate of the rating the user might give to that movie.

for eg:
given 
Nu-- Number of users
Nm-- number of movies

R(i,j) --- rating give by ith user of jth movie

we can find out X(X1,X2,X3...) the  variables defining the inherent values of the movie
Also we can find out Wi,bi -- Weights of the ith user

such that Wi*Xj +bi = R(i,j)

if Distance between Xi and Xi+n is less then i and i+n movie are similar to each other

ð½(ð±(0),...,ð±(ð‘›ð‘šâˆ’1),ð°(0),ð‘(0),...,ð°(ð‘›ð‘¢âˆ’1),ð‘(ð‘›ð‘¢âˆ’1))=
[ 12âˆ‘(ð‘–,ð‘—):ð‘Ÿ(ð‘–,ð‘—)=1(ð°(ð‘—)â‹…ð±(ð‘–)+ð‘(ð‘—)âˆ’ð‘¦(ð‘–,ð‘—))2]   +   [ðœ†2âˆ‘ð‘—=0ð‘›ð‘¢âˆ’1âˆ‘ð‘˜=0ð‘›âˆ’1(ð°(ð‘—)ð‘˜)2+ðœ†2âˆ‘ð‘–=0ð‘›ð‘šâˆ’1âˆ‘ð‘˜=0ð‘›âˆ’1(ð±(ð‘–)ð‘˜)2] -- regularization

****actual impementation of cost function***

def cofi_cost_func_v(X, W, b, Y, R, lambda_):
    """
    Returns the cost for the content-based filtering
    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.
    Args:
      X (ndarray (num_movies,num_features)): matrix of item features
      W (ndarray (num_users,num_features)) : matrix of user parameters
      b (ndarray (1, num_users)            : vector of user parameters
      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies
      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user
      lambda_ (float): regularization parameter
    Returns:
      J (float) : Cost
    """
    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R
    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))
    return J

****Implementing custom gradient decent with adam optemizer using tensor flow*****
TensorFlow has the marvelous capability of calculating the derivatives for you. This is shown below. 
Within the tf.GradientTape() section, operations on Tensorflow Variables are tracked. When tape.gradient() is later called, 
it will return the gradient of the loss relative to the tracked variables. 
The gradients can then be applied to the parameters using an optimizer.

#  Useful Values
num_movies, num_users = Y.shape
num_features = 100

# Set Initial Parameters (W, X), use tf.Variable to track these variables
tf.random.set_seed(1234) # for consistent results
W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')
X = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')
b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')

# Instantiate an optimizer.
optimizer = keras.optimizers.Adam(learning_rate=1e-1)

iterations = 200
lambda_ = 1
for iter in range(iterations):
    # Use TensorFlowâ€™s GradientTape
    # to record the operations used to compute the cost 
    with tf.GradientTape() as tape:

        # Compute the cost (forward pass included in cost)
        cost_value = cofi_cost_func_v(X, W, b, Ynorm, R, lambda_)

    # Use the gradient tape to automatically retrieve
    # the gradients of the trainable variables with respect to the loss
    grads = tape.gradient( cost_value, [X,W,b] )

    # Run one step of gradient descent by updating
    # the value of the variables to minimize the loss.
    optimizer.apply_gradients( zip(grads, [X,W,b]) )

    # Log periodically.
    if iter % 20 == 0:
        print(f"Training loss at iteration {iter}: {cost_value:0.1f}")

***********Renforcement Learning****************

Unlike supervised learning RL tells the object what to do rather than how to do it
driven by rewards and maximizing good rewards rather than otimization funtion.

Total rerturn at a given step is calculated sum of Future discounted rewards. Discounted by discount factor
Immideate reward is not discounted whereas the further down the road reward is for the agent lower is it value
when view at current time.
This Philosophy can be equated with financial market where money gets subjected to inflation in the future.
So ! Rupee earned now is of the same value where as a 1 Ruppe you are getting in the future is of less value 
when compared to if you get it now

Return = R1 + Alpha*R2 + Alpha^2 R3 .......

In case of -ve rewards the algorithm optimizes by pushing out -ves as far into the future as possible.
An analagy is if you have been asked to pay 10$ now and deffer this to pay may years later , given the inflation factor 
its much economical to pay 10$ in future than paying it now

The goal of reinforcement learning is to find a policy Pi or Pi of S that tells you what 
action to take in every state so as to maximize the return
