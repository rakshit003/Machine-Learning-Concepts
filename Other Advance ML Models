************Decision Tree************

****Classification Decision Tree****

Idea is to maximize Purity during everfy node split. Purity means increasing the presense of a given class in the mode.
Or minimize Impurtity(Entropy). Hence close the mix of 2 classes is to 0.5 its more impure and if a fraction of presence of 
one class close to 1 or close to 0 makes it more pure and reduce entropy

Decision tree is a recursive alogirthm instead of gradient decent learning

Hence in decision tree we minimize Entropy (Impurity fucntion) instead of cost function to learn the model
Entropy= -plog(p) - (1-p)log(1-p).. if we plot this its a inverted parabola with peak at p=0.5
p is the fraction of instance of a class

Even Gini function also looks like this

In actual decision tree, entroy of both the nodes are done a weighted average so the bigger sample node gets more importance
and the informaation gain is caculated by butracting the root node entropy with the wirghted average chid node entropy

Information gain is calculated firs for every feature and then among different feature to decide
which feature and what value of the feature should be used for the split


if the ctaegorical feature has multiple values then its better to use one hot encoding, which means for very category
we create a new feature and mark it as 1 where that category exist and 0 where is does not.
for a give row one of the feature will have a value 1, which means that feature is hot for the record.
Hence the name one hot encoding.Advantage is for spliting a node it will make a binary decision for each feature

This is also work with NN or Logistic or linear regression training

In case of continuous feature, data of n points, choose n-1 midpoints between n successive points and find
information gain at that point, as a result fin the midpoint with the most information gain


*****Regression Decision Tree*****

In Regression tree the value of a node is decised by the average value of the priction numeri variable of the samples in that node
for learning process Instead of Entropy(impurity) , variance of the node is used as a threshold. feature which results in least
weighted variance is use to decide.

And instead of information gain from root node. variance lost between is used as a measure by algorithm

***** Ensamle of Trees****

Issue with single decision tree is that even if change make changes to minority of examples. the tree scruture can change
as a new feature can become the main root node of the tree . Hence its very sensitive of change in input.
Where as if we train a bunch of decision trea and use a voting classifier to get the predictions , it turns out to be more
stable.

*****Method== Sampling with Replacement*****
create multiple data set for different tree by fecthing samples with replacement (this will result in duplicate training examples which is fine)
As the data set changes ,with that the feature of the node also change as the new entropy might be different because of the new variation in dataset

multiple ways to ensamble

Random Forrest: 
Sample with replacement and create same size dataset B times to create B trees.  B  typically could be 64-128.
This is also called Bagged Decision Tree. This concept is further extended by not chosing from all features avialable
but instead only choose from n<N features becasue this will help in creating different type of decision trees.

XG Boost: Xtreme gradient boosting
Idea is to instead of picking the samples in the new mag randomly, give more probablity of picking to 
miscalssified examples from periously built trees, hence the subsecquent tree built will have training set of more samples of misclassified 
records from ensamle of built so far. This process is called boosting

XG boost is very good for prediction in lines with NN


*****Clustering*******

Clustering algorithm looks at a set of data points and automatically finds datapoints that are related to each other
Use cases: grouping similar articales together, Analyse DNA data to group people into similar traits, Astronomers use to group
bodies in space

K means Clsutering: 
-randomly pick multiple centors of the clusters
-It does 2 things repeatedly , 1 Assign each point to centroids , 2 move/update the cluster centroids

Assigning points to cluster centroids is decided by how closer point is to which cluster
After the assign of points to initial centroids , an average of each group is taken and that becomes the new centroid

Above 2 points are repeated
Idea is afet multiple iterations when there is no further changes to location of centroids
, algorithm will converge the true centroids and group similar clusters together

Cost function is nothing but the avergae of distance error of each point with their centroids
