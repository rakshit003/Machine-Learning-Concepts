*****vectorization******

f= w1x1+w2x2.....wnxn + b
can be implemented as dot prodcut of w and x
f=w.x+b
dot prduct is nothing but sum of product of corresponding numbes in each vectors.

this is much more efficent to implement and can be implemented in parellal with help of GPUs
or else we have to implement it using for loops which is much slower. like below

for j in range(0 to 16 features):
 f= f w[j]*x[j]

How Vectorization is faster?
multiplication of numbers can be performed in parellel and laters summed up in a single step.
Hence it scales well with large dataset

Where as for loop does acummulated sum, where in every loop iteration gthe accumulated count gets
updated

*******Scaling of variables***********

How does scaling impact ?

f= w1x1 + w2x2 + b

if x1 range from 1-6 (like number of bedrooms)
and x2 range from 200-3000 (like size of the house)

then upon learning the weights of x2 would be relative much smaller than weights of x1 inorder to arrive at the right output value.
This impacts the gradient decent as below

If we plot a contoure plot wrt these variables. We will see an oval shaped area which has a huge height and very narrow width.
as height is resultant of ranges in x2 and width in small range in x1.
which means a small change in x1 result in a large change in f (or even the cost function) where as a small change in x2
provides very little change to f(or the cost function) , which means slope of the curve wrt x2 is very gradual when compared
to x1. As a result while running gradient dec(decent it will take a long time to reach the minima as the next steps keep on jumping
back and forth before reaching the minima(because if slope of x1 is large it can result in overshooting).
This will make GD to run slowly

techniques to scale

mean normalization --> (x- Mean)/max-min
Z normalization -->  (x- Mean)/ Sigma(SD)

generally , features should scale somewhere around -1 to 1 or even -3 to +3 is also fine
If a features is naturaaly lying beftween these values , it might not need scaling, although no harm in in scaling
like -100 to 100 or -.0001 to 0.00001 would need to be rescaling

***Feature engineering ***

Using intuition to combine features together to build a more reliable feature for the model
Polynomial regresion involves manipulating feature by increasing the polynomial degree of the feature.

Note, feature scaling becomes very important when doing polunomial regression as the relative range will vary 

**Polynomial features

sometimes relationship of output with a variable is not linear
for eg: house price are related to its area , but maybe it penalizes very small houses or may not give enough price to very large houses
as well.which means the linear effect will taper off if area is very small or very large.
In that case a polynomial area feature would give a better result.

If we keep on adding multiple polynomial degrees in the equation , gradient will automaitcally figure out to assign the right weight
to a specific polynomial term and if run for a longer time other polynomial might get zero weights.
Gradient descent is picking the 'correct' features for us by emphasizing its associated parameter

Let's review this idea:

-Intially, the features were re-scaled so they are comparable to each other
-less weight value implies less important/correct feature, and in extreme, when the weight becomes zero or very close to zero, the associated feature is not useful in fitting the model to the data.
-if lets say the weight associated with the  ğ‘¥2 feature is much larger than the weights for ğ‘¥ or ğ‘¥3 it is the most useful in fitting the data.

Another way to think about it is, Ouput is Linearly related to x^2 instead of x. hence it will assign more weight to x^2 feature.

Even very complex function like cos/sign  can be modeled with gradient decent provided enough level of polynomial degrees
are aviablae in the input output equation.
Which means even if the underlying relationship between input and output data is very complex.
GD is capable enough to model the parameters to match the relationship equation.

**********Regularization****************

Regularization helps you avoid overfitting by keeping the weights of the variables in check without totally eliminating the variable
Hence you don't need to do a feature selection and regularization can work but itself by reducing an imapct of a variable by tweeking its 
weights automatically
For eg, if the weight of a polynomial variable is very large which is causing the overfitting, by adding a regularization 
term in the cost function . It will help tweek the value of that parameter to very low
Its not neccessary to regularize 'b' parameter

Alternate to regularization to avoid overfitting is by increadsing data or feature selection.

Regularization is done by adding lambda parameter which parameterize the (sum of square of all weights except b)

If lambda is zero then we have no overfitting, but if lambda is very high. then GD will make all values of parameter ~ 0
Hence it will underfit

So when GD is minimizing , the first term will help in minimizing the error as usual , while the second(lambda) term will help 
in minimizing overfitting by keeping the weights values in check. Because if weights are high which makes their squared 
term high and increase the overall cost. Hence in order to keep the cost function low, GD should reduce the error 
while keeping the weight size low

 L2 regularization takes the square of the weights, so the cost of outliers present in the data increases exponentially. 
L1 regularization takes the absolute values of the weights, so the cost only increases linearly. 

L2 is more robust but will cost efficent in calculation

ğ½(ğ°,ğ‘)=12ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))2+ğœ†2ğ‘šâˆ‘ğ‘—=0ğ‘›âˆ’1ğ‘¤2

***** Gradient Decent in Tensor Flow or Auto Div ********
In Order to perform gradient decent , We need to get the derivatives of the cost function, which are used to 
iteratively update the parameters untill we get the optimized value of the parameter

There is a way to instruct tensor flow to find the derivatives by providing cost function
The function used is tf.GradientTape()

w= tf.Variable(3) **3 is default value , it can be changed to anything
x=1
y=1
alpha=0.01

for i in range(100):

 with tf.GradientTape() as Tape:
  fwb = w*x
  costJ= = (fwb-y)**2
 
 [dJdw]= Tape.Gradient(costJ,[w])

 w.assign_add(-alpha*dJdw)




