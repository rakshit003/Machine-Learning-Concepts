***Gradient Decent****

Gradient Decent helps you arrive at the cost value of a cost function but adjusting its parmaters in stepwise systematic way.
J(W,b) function needs to be minimized with respect to parameters W(w1,w2,w3..wn) and b. Gradient decent uses derivatives at
each step to indentify the steepest decent at that moment and update the parameter most which causes the steepest decent

For linear regression J(w,b) is a squared error function hence its shape is a bow or hammock shape, which always have a single minima
for J(w,b) will not always have a bow/hammock shape , as a result it could have local and global minima as well. 
In Nural Net we get multiple minimas in cost function

for eg the image of a hill range(or a golf park) like structure in a 3d plot is not a squared error cost function since it has many rough and troughs
like we get in nural net.

At a give moment in the park , gradient decent asks in what direction should i take the next baby step in order to decent the steepest.
Hence based on where we start , we can endup in different local minimas in a gradient decent algorithm if we don't have a bow shape cost function.

in every step Parmeter gets updated in below manner

Wnext = Wold - & d/dw (J(W,b)) -- partial derivate
Bnext = Bold - & d/dw (J(W,b)) -- Partial Derivate

where &(alpha) is learning rate, which determines how bigh of a step we take the baby step downhill
d/dw is the derivative term with respect to cost function. This also determines the direction and size of the baby step

Note: All the parameters get updated parellaly(simultaneos update) which means B gets updated with old W values in place abd vice versa

After implementing multiple steps , we can stop the process if J doesn't change as much to cause a significant difference


