***Gradient Decent****

ref: https://github.com/rakshit003/Machine-Learning-Concepts/blob/main/reference%20files/C1_W1_Lab04_Gradient_Descent_Soln.html

Gradient Decent helps you arrive at the cost value of a cost function but adjusting its parmaters in stepwise systematic way.
J(W,b) function needs to be minimized with respect to parameters W(w1,w2,w3..wn) and b. Gradient decent uses derivatives at
each step to indentify the steepest decent at that moment and update the parameter most which causes the steepest decent

For linear regression J(w,b) is a squared error function hence its shape is a bow or hammock shape, which always have a single minima
for J(w,b) will not always have a bow/hammock shape , as a result it could have local and global minima as well. 
In Nural Net we get multiple minimas in cost function

for eg the image of a hill range(or a golf park) like structure in a 3d plot is not a squared error cost function since it has many rough and troughs
like we get in nural net.

At a give moment in the park , gradient decent asks in what direction should i take the next baby step in order to decent the steepest.
Hence based on where we start , we can endup in different local minimas in a gradient decent algorithm if we don't have a bow shape cost function.

in every step Parmeter gets updated in below manner

Wnext = Wold - & d/dw (J(W,b)) -- partial derivative
Bnext = Bold - & d/dw (J(W,b)) -- Partial Derivative


where d/dw: 
is the derivative term with respect to cost function. This also determines the direction and size of the baby step
This derivative is nothing but the slope of the curve at that point time. If bigger is the slope the amplitude of slope (y/x) is higher
which means that parameter will get updated the with biggest value whoes slope is the highest. Hence every parameter will get shifted
by different amounts depending on what slope they have in the cost function. So gradient decent auto adjust the paremeter decrement based
on value of derivate of that parameter

and where &(alpha):
is learning rate, which determines how bigh of a step we take the baby step downhill. & helps in amplifying the step size chosen 
by the derivative in that direction.

Note: All the parameters get updated parellaly(simultaneos update) which means B gets updated with old W values in place abd vice versa

After implementing multiple steps , we can stop the process if J doesn't change as much to cause a significant difference


**Further explaination***
there are few things to note in gradient decent Algorithm

The gradient decent by design takes smaller baby as we approach the local minimun because the slope keeps on reducing. Hence
it it more iterations to reach if you are close to local minimum than if you are very far from minimum and want to reach closer
to local minimun, Even if &(learning rate) is kept to a fixed value. Hence , the intial steps taken by gradient decent are very 
large since the gradient is high and last many steps are small(and smaller is the cost reduction at the end).

If &(Learning rate) is too small then the baby steps are very small and it takes a lot of iterations to reach to local minima.
And if & is too large , baby steps are large and there are chances it might overshoot the local minima and if it reaches a higher
slope point then the next step is even larger and ultimately the Cost keeps on increasing and algoritm might even diverge instead
of converging

Also, if you are in some local minima it is natuarlly hard for gradient decent o cross the local minima as the algorithm 
has a narrow view of the curve and might think the local minima is the globala minima. for eg. if it reaches local minima
then slope will be 0 . which means with every iteration step w will not update any further as derivate will be 0 always.

During learning if we see a weight switching it sign in evry iteration then it means it is ossiclating across the minima.
Hence we need to decrease the &(learning rate) to let it not oscliate. One reason of oscilalations could be no scalling
done of features
If for a weight its derivate is reducing without changing its sign and eventually ending up in almost zero value,
 then it seems to be converging for that gradient

**
one way to check GD is converging is by plotting cost function against internal iterations of GD.
Cost function J should decrease after every single iteration if learning rate is small, if it increases the &(alpha) is chosen poorly.
You can also set a threshold epsilon(~0.001) . if decrease in cost between iteration is epsilon then stop convergence.
Even after setting alpha to very very small the J is not decreasing this means there is something problem with code.

One techniqure is to start with leanring rate = 0.001, 0.003 , 0.01 ... (multiples of 3)
and then plot Learning curve so you find the value that is too small and the value is too large and then find the largest
possible value which results in decreasing J so alorithm can converge faster.

**Batch vs Stocastic Gradient Decent****

In Batch GD, for every iterations we find the derivate  by summing up for all training examples instead of a subset of training data
whereas in Stocastic GD inorder to make it run faster we find derivate by summing only a random subset of training examples
In Fact Pure Stocastic GD only use 1 example in each iteration. The one that uses mini batch is called mini-batch SGD
Baby steps becomes jerky(as the graient is jerky) and it could help to overshoot local minima as well
