One of the most basic questions we might ask of a model is: What features have the biggest impact on predictions?

This concept is called feature importance.

There are multiple ways to measure feature importance. One of the most widely used technique is permutation importance. Shown as below


**technique 1 *** permutation importance

How It Works ?

Permutation importance is calculated after a model has been fitted
If I randomly shuffle a single column of the validation data(shuffle the values across the column), leaving the target and all other columns in place, 
how would that affect the accuracy of predictions in that now-shuffled data?

Randomly re-ordering a single column should cause less accurate predictions,
Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions.

Steps:

Step1 :Get a trained model.
Step2: Shuffle the values in a single column, make predictions using the resulting dataset. 
       Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. 
       That performance deterioration measures the importance of the variable you just shuffled.
Step3: Return the data to the original order (undoing the shuffle from step 2). 
       Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column.



****code*****
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())
*************

The values towards the top are the most important features, and those towards the bottom matter least.

The first number in each row shows how much model performance decreased with a random shuffling 
(in this case, using "accuracy" as the performance metric).
The number after the Â± measures how performance varied from one-reshuffling to the next.

Why Are These Insights Valuable
These insights have many uses, including

***technique 2 -  Partial dependent plots******
***techique 3 - Shap values**********

Debugging
Informing feature engineering
Directing future data collection
Informing human decision-making
Building Trust

Feature Enginnering
- IF 2 features are high importance and closely entangled, they can be combined with some logic to create a new feature



************Diagnostics***********

Once the modelparameters are fit the training set the training error(Jtrain cost function error) is likely to be lower
than the generalized error(Avg error on New examples)

For a regression problem J(error) on test can be calculated by running the test set on Cost function
where as for Classification problem a better way to find the error is miscalssification ratio from the output

Jlinear= sklearn.mean_squared_error(y_train, yhat) / 2}

Best Practice is to build 3 sets- Training , cross validation(or validation/dev set) and test set
Cross validation is used to check the best model among multiple models. It is also used for hyper parameter tuning  or different
different architecture of NN
Error is called -  cv error or dev error

# Compute the training MSE
yhat = model.predict(X_train_mapped_scaled)
print(f"Training MSE: {mean_squared_error(y_train, yhat) / 2}")

# Compute the cross validation MSE
yhat = model.predict(X_cv_mapped_scaled)
print(f"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}")

generalization error reporting: this is always the test set error for the data which the model ihas never seen

training set - used to train the model
cross validation set (also called validation, development, or dev set) - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset.
test set - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models.

# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.
x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)

# Split the 40% subset above into two: one half for cross validation and the other for the test set
x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)




****Feature Scaling***********
In the previous course of this specialization, you saw that it is usually a good idea to perform feature scaling to help your model converge faster. 
This is especially true if your input features have widely different ranges of values.

As with the training set, you will also want to scale the cross validation set. An important thing to note when using the z-score is you have to use the mean and standard deviation of the training set when scaling the cross validation set. This is to ensure that your input features are transformed as expected by the model. One way to gain intuition is with this scenario:

Say that your training set has an input feature equal to 500 which is scaled down to 0.5 using the z-score.
After training, your model is able to accurately map this scaled input x=0.5 to the target output y=300.
Now let's say that you deployed this model and one of your users fed it a sample equal to 500.
If you get this input sample's z-score using any other values of the mean and standard deviation, then it might not be scaled to 0.5 and your model will most likely make a wrong prediction (i.e. not equal to y=300).

# Initialize the class
scaler_linear = StandardScaler()

# Compute the mean and standard deviation of the training set then transform it
X_train_scaled = scaler_linear.fit_transform(x_train)

# Scale the cross validation set using the mean and standard deviation of the training set
X_cv_scaled = scaler_linear.transform(x_cv)

# using training set scalar for test set
X_test_mapped_scaled = scaler.transform(X_test_mapped)

*****Choosing the best model****

When selecting a model, you want to choose one that performs well both on the training and cross validation set. 
It implies that it is able to learn the patterns from your training set without overfitting.
Plot the Training error and Cv Error in y axis and different model param values(like degree of polynomial) in x axis
and find the point where cve is minimum

You can then publish the generalization error by computing the test set's MSE

eg of publication:
Training MSE: 47.15
Cross Validation MSE: 79.43
Test MSE: 104.63

In the previous sections on regression models, you used the mean squared error to measure how well your model is doing. 
For classification, you can get a similar metric by getting the fraction of the data that the model has misclassified. 
For example, if your model made wrong predictions for 2 samples out of 5, then you will report an error of 40% or 0.4



********Bias vs Variance******

if Jtrain is high anf Jcv is low then its high bias(underfitting)
if Jtrain is low and Jcv is high the its hig variance(overfitting)
also as the complecity increase Jcv decrease intially(as bias reduces) and then Jcv increase(variance increase).
the idealy point is when Jcv is the least

One good way to deal with high variance is regularization.
Lambda is the parameter which decides this, in regularization section of the cost function.
This helps to keep the size of weights low while fitting the data well in the model.
for eg if Lambda is very large then the regularization term will try to keep the size of weights very low, 
therby making the model having more bias compared to variance(underfit model). a very high lamda will result in a flat line(w=0)

the other extreme lamda=0 will be verfitting as there is no regularization

then lamda= intermediate value

Hence cross validation can be used to go over different values of lamda and find the best value for test set
A good way to vary lamda is to start small(0.01) and keep doubling it. This can bes use to measure Jtrain or Jcv

**judging learnig algo performance**

calculate below 3 erros

Human level error(baselines(existing algo) error):
Training error 
Cross validation error

Now check difference between Training error - Human error --- if this is high then we have hig bias problem(underfitting)
Now check difference between Traing error and Cv Error --- if this is high then we have high variance problem(overfitting)

eg:

High Vairance
if Baseline - trainig = 0.2%
if Training - Cross V = 4%

High Bias
if Baseline - trainig = 4.4%
if Training - Cross V = 0.5%


***learning Curves***

This plots Jtrain and Jcv against size of the data set

- Its interesting to see that as size of data set increase training error also increases, 
   because its easy to fit a function on less data point when compared to more data points 
- And if size of data set increase CV Error reduces
- J Cv is normally higher than the ztraining error


