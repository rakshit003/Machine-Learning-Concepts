One of the most basic questions we might ask of a model is: What features have the biggest impact on predictions?

This concept is called feature importance.

There are multiple ways to measure feature importance. One of the most widely used technique is permutation importance. Shown as below


**technique 1 *** permutation importance

How It Works ?

Permutation importance is calculated after a model has been fitted
If I randomly shuffle a single column of the validation data(shuffle the values across the column), leaving the target and all other columns in place, 
how would that affect the accuracy of predictions in that now-shuffled data?

Randomly re-ordering a single column should cause less accurate predictions,
Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions.

Steps:

Step1 :Get a trained model.
Step2: Shuffle the values in a single column, make predictions using the resulting dataset. 
       Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. 
       That performance deterioration measures the importance of the variable you just shuffled.
Step3: Return the data to the original order (undoing the shuffle from step 2). 
       Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column.


****code*****
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())
*************

The values towards the top are the most important features, and those towards the bottom matter least.

The first number in each row shows how much model performance decreased with a random shuffling 
(in this case, using "accuracy" as the performance metric).
The number after the Â± measures how performance varied from one-reshuffling to the next.

Why Are These Insights Valuable
These insights have many uses, including

***technique 2 -  Partial dependent plots******
***techique 3 - Shap values**********

Debugging
Informing feature engineering
Directing future data collection
Informing human decision-making
Building Trust

Feature Enginnering
- IF 2 features are high importance and closely entangled, they can be combined with some logic to create a new feature



************Diagnostics***********

Once the modelparameters are fit the training set the training error(Jtrain cost function error) is likely to be lower
than the generalized error(Avg error on New examples)

For a regression problem J(error) on test can be calculated by running the test set on Cost function
where as for Classification problem a better way to find the error is miscalssification ratio from the output

Best Practice is to build 3 sets- Training , cross validation(or validation/dev set) and test set
Cross validation is used to check the best model among multiple models. It is also used for hyper parameter tuning  or different
different architecture of NN
Error is called -  cv error or dev error

generalization error reporting: this is always the test set error for the data which the model ihas never seen

training set - used to train the model
cross validation set (also called validation, development, or dev set) - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset.
test set - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models.

# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.
x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)

# Split the 40% subset above into two: one half for cross validation and the other for the test set
x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)
